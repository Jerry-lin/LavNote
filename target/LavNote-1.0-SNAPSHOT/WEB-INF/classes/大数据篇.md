# 大数据知识篇

## 离线处理篇

### Zookeeper

zookeeper是分布式协调系统，主要的功能有两个：

1.管理用户程序提交的数据(状态数据)

2.为用户程序提供数据节点监听服务

Zookeeper的选举流程：

**1.如果是全新的集群：**

以一个简单的例子来说明整个选举的过程.
	假设有五台服务器组成的zookeeper集群,它们的id从1-5,同时它们都是最新启动的,也就是没有历史数据,在存放数据量这一点上,都是一样的.假设这些服务器依序启动,来看看会发生什么.
	1) 服务器1启动,此时只有它一台服务器启动了,它发出去的报没有任何响应,所以它的选举状态一直是LOOKING状态
	2) 服务器2启动,它与最开始启动的服务器1进行通信,互相交换自己的选举结果,由于两者都没有历史数据,所以id值较大的服务器2胜出,但是由于没有达到超过半数以上的服务器都同意选举它(这个例子中的半数以上是3),所以服务器1,2还是继续保持LOOKING状态.
	3) 服务器3启动,根据前面的理论分析,服务器3成为服务器1,2,3中的老大,而与上面不同的是,此时有三台服务器选举了它,所以它成为了这次选举的leader.
	4) 服务器4启动,根据前面的分析,理论上服务器4应该是服务器1,2,3,4中最大的,但是由于前面已经有半数以上的服务器选举了服务器3,所以它只能接收当小弟的命了.
	5) 服务器5启动,同4一样,当小弟.

**2.如果不是全新的集群：**

那么，初始化的时候，是按照上述的说明进行选举的，但是当zookeeper运行了一段时间之后，有机器down掉，重新选举时，选举过程就相对复杂了。

需要加入数据id、leader id和逻辑时钟。

数据id：数据新的id就大，数据每次更新都会更新id。

Leader id：就是我们配置的myid中的值，每个机器一个。

逻辑时钟：这个值从0开始递增,每次选举对应一个值,也就是说:  如果在同一次选举中,那么这个值应该是一致的;  逻辑时钟值越大,说明这一次选举leader的进程更新.

选举的标准就变成：

​        1、逻辑时钟小的选举结果被忽略，重新投票               		

​        2、统一逻辑时钟后，数据id大的胜出

​        3、数据id相同的情况下，leader id大的胜出

根据这个规则选出leader。

### HDFS

#### HDFS的文件读取过程

![HDFS读程序](/Users/youyujie/Documents/Java知识点复习图片/HDFS读程序.png)

读流程过程如下：

​	1.客户端和NameNode通信，返回存储请求数据的datanode服务器

​	2.客户端按照就近原则，与datanode通信，建立socket流

​	3.datanode开始发送数据，这里是以packet为数据单位进行通信，然后做相关的数据校验

​	4.客户端以packet为单位接受数据，缓存在本地，写入到目标文件。

#### HDFS的文件写入过程

![image-20180529190832148](/Users/youyujie/Documents/Java知识点复习图片/HDFS写流程.png)

写流程如下：

​	1.客户端向namenode请求上传数据。

​	2.namenode检查客户端请求的路径是否存在该文件，如果存在相关的文件，抛出异常，否则返回可以上传。

​	3.客户端接受namenode返回的信息后，请求发送第一个block块。

​	4.namenode根据相关算法，主要根据距离，返回特定的datanode。

​	5.客户端收到namenode的返回结果后，向datanode请求建立socket channel 连接

​	6.第一个datanode和其他datanode建立连接通道，这个通道叫做PIPE LINE

​	7.相关的datanode返回建立socket连接成功。

​	8.客户端收到连接建立后，等待数据发送

​	9.客户端发送数据，发送的基本单元是packet，这是64KB大小

​	10.datanode本地缓存，然后进行相关的数据校验，然后将packet发送给存储副本的datanode

​	11.在完成第一个block后，后来的block发送重复上述步骤

#### CheckPoint过程

![secondarynamenode元数据checkpoint机制](/Users/youyujie/Documents/个人简历/day07/secondarynamenode元数据checkpoint机制.png)

流程如下：

​	1.namenode请求secondary namenode是否需要checkpoint，看是否这时候可以checkpoint。

​	2.secondary namenode向namenode请求checkpoint。

​	3.如果此时正在进行写日志，这个时候将正在写的edits文件进行滚动。

​	4.secondary namenode将edits文件和fsimage文件下载到本地，注意fsimage只会在第一次下载，这个时候fsimage不会很大，以后就会只下载edits文件。

​	5.secondary namenode 会将fsimage和edits文件加载到内存，然后将在这些操作重放一边。

​	6.最后将这些合并好的元数据dump到fsimage .checkpont文件。

​	7.secondary namenode会将新的fsimage .checkpont上传到namenode。

​	8.重命名fsimage，然后将旧的edits文件清除。

### MapReduce

#### 任务提交流程

![image-20180529193954106](/Users/youyujie/Documents/Java知识点复习图片/MapTask并行度决定机制.png)

具体流程如下：	

​	1.在job提交之后，会执行job .submit(),这个方法会使用JobSubmitter类，这个类中又一个成员变量cluster，内部形成一个代理对象。

​	2.如果这个任务在本地运行，那么返回的是LocalJobRunner，如果任务在集群上运行，返回的是YarnJobRunner。

​	3.yarn或者本地返回目录StagingDir，然后MapReduce框架将会返回给任务一个jobId。

​	4.程序拿到目录后将目录和JobId进行拼接，组成相关新的文件目录。

​	5.FileInputFormat使用getSplits得到List<Splits>, 形成job.splits，然后会的job.xm,jar最后将这三个文件拷贝到新的路径。

#### MapReduce的切片机制

![image-20180529203139311](/Users/youyujie/Documents/Java知识点复习图片/MapReduce切片机制.png)

maptask任务分配切片机制](/Users/youyujie/Documents/个人简历/day08/maptask任务分配切片机制.png)

**需要注意的是MapReduce的切片只是一个逻辑程序。切片是用户程序在做。**一个切片对应一个MapTask实例

通过分析源码，在FileInputFormat中，计算切片大小的逻辑：Math.max(minSize, Math.min(maxSize, blockSize));  切片主要由这几个值来运算决定

| minsize：默认值：1               配置参数： mapreduce.input.fileinputformat.split.minsize |
| ------------------------------------------------------------ |
| maxsize：默认值：Long.MAXValue           配置参数：mapreduce.input.fileinputformat.split.maxsize |
| blocksize                                                    |

因此，**默认情况下，切片大小****=blocksize**

maxsize（切片最大值）：

参数如果调得比blocksize小，则会让切片变小，而且就等于配置的这个参数的值

minsize （切片最小值）：

参数调的比blockSize大，则可以让切片变得比blocksize还大

#### ReduceTask的并行度机制

reducetask的并行度同样影响整个job的执行并发度和执行效率，但与maptask的并发数由切片数决定不同，Reducetask数量的决定是可以直接手动设置：

//默认值是1，手动设置为4

job.setNumReduceTasks(4);

#### Shuffle过程

![mapreduce的shuffle原理](/Users/youyujie/Documents/个人简历/day09/mapreduce的shuffle原理.png)

具体流程如下：

​	1.MapTask在完成逻辑处理后，将数据写到OutputCollector，然后OutPutCollector组件将分数据放到环形缓冲区。

​	2.环形缓冲区默认大小是100MB，80%是数据区，20%是用于排序；在环形缓冲区需要溢出数据之前，调用Partitioner组件，根据key分区，然后使用快排和外部排序根据key排序，如果这里有Combiner组件，将会根据相同的Key合并，然后线程spiller将数据溢出到文件，溢出的文件分区且有序。

​	3.进过多次溢出，会产生很多小的文件，这里可能进行多次文件合并，使用的算法是归并排序，如果这里有Combiner组件，将会根据相同的Key合并。

​	4.ReduceTask根据自己的分区号，在各个MapTask机器中，获取到属于自己的数据，然后采用的归并排序合并各个MapTask机器的分区，这里可以使用GroupComparaor(k,nextK)判断数据是否属于一组，默认传递第一个key，然后一组的数据的value组成一个迭代器。

#### MapReduce全流程

![mapreduce原理全剖析--段氏六脉神剑](/Users/youyujie/Documents/个人简历/day09/mapreduce原理全剖析--段氏六脉神剑.png)

具体流程：

​	1.MapTask使用组件InputFormat，默认的实现类事TextInputFormat，内部使用RecordRead，调用read()方法，从文件中一行一行的读取数据。

​	2.MapTask根据程序逻辑处理数据，将数据输出，数据将会到达OutPutCollector中，然后OutPutCollector组件将分数据放到环形缓冲区。

​	3.环形缓冲区默认大小是100MB，80%是数据区，20%是用于排序；在环形缓冲区需要溢出数据之前，调用Partitioner组件，根据key分区，然后使用快排和外部排序根据key排序，如果这里有Combiner组件，将会根据相同的Key合并，然后线程spiller将数据溢出到文件，溢出的文件分区且有序。

​	4.进过多次溢出，会产生很多小的文件，这里可能进行多次文件合并，使用的算法是归并排序，如果这里有Combiner组件，将会根据相同的Key合并。

​	5.ReduceTask根据自己的分区号，在各个MapTask机器中，获取到属于自己的数据，然后采用的归并排序合并各个MapTask机器的分区，这里可以使用GroupComparaor(k,nextK)判断数据是否属于一组，默认传递第一个key，然后一组的数据的value组成一个迭代器。

​	6.ReduceTask接受到任务后，进行逻辑业务处理，然手使用OutputFormat，默认的实现类是TextOutPutFormat，这里面使用RecordWriter的write()方法，将数据写到文件系统。

#### Yarn在任务提交流程的作用

![mapreduce&yarn的工作机制----吸星大法](/Users/youyujie/Documents/个人简历/day09/mapreduce&yarn的工作机制----吸星大法.png)

具体流程如下：

​	1客户端通过.YarnRunner提交任务，向ResourceMananger申请运行一个程序。

​	2.ResourceManange返回一个提交程序的路径和jobID

​	3.客户端根据路径和JobId拼接称为一个新的路径，然后将job.xml,job.split和jar包

​	4.客户端通知RM资源提交完成，申请运行MRAPPmaster。

​	5.RM将请求封装称为Task，放入任务调度队列。

​	6.nodeManager领取到任务，然后产生一个容器Container，从HDFS将job资源下载到本地，启动MRAppmaster。

​	7.MRAppMaster向RM申请MapTask运行的容器。

​	8.nodeManager领取到mapTask任务，然后分配容器，这个容器叫做yarnChild，下载job资源。

​	9.MRAppmater发送启动maptask任务的命令,maptask启动运行，任务会被MRAppmaster监控。

​	10.maptask运行完成后，MRAppMaster向RM申请运行ReduceTask任务。

​	11.nodemanager领取到任务后，分配容器，启动ReduceTask，然后向nodemanger要maptask处理后的数据，然后拉去数据，执行完成。

​	12.当全部运行完之后，MRAppMaster会向RM注销自己。

#### NameNode安全模式

![关于namenode的安全模式](/Users/youyujie/Documents/个人简历/day11/关于namenode的安全模式.png)

何为安全模式？

​	1.namenode刚在启动时，内存中有文件和文件的块id以及副本数量，不知道块所在的datanode

​	2.namenode需要等待所有datanode向他汇报自身持有的块信息，namenode才能在在元数据中补全文件块信息中的位置信息。

​	3.只有当namenode找到99.8%的块的位置信息，才会推出安全模式，正常对外服务。

#### HA机制

### Hive

​	Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供类SQL查询功能。

#### Hive架构

![image-20180529224054724](/Users/youyujie/Documents/Java知识点复习图片/hive架构图.png)

#### UDF开发实例

```java
package cn.itcast.bigdata.udf
import org.apache.hadoop.hive.ql.exec.UDF;
import org.apache.hadoop.io.Text;

public final class Lower extends UDF{
	public Text evaluate(final Text s){
		if(s==null){return null;}
		return new Text(s.toString().toLowerCase());
	}
}
```

2、打成jar包上传到服务器

3、将jar包添加到hive的classpath

​	hive>add JAR /home/hadoop/udf.jar; 	

4.创建临时函数与开发好的java class关联 

​	Hive>create temporary function toprovince as 'cn.itcast.bigdata.udf.ToProvince'; 

#### 分桶和分区的区别

分区：	

​	Hive可以在创建表的时候指定分区空间，这样在做查询的时候就可以很好的提高查询的效率。分区在HDFS路径表示为一个文件夹，文件夹时分区的字段。

分桶：

​	分桶是比分区更小的粒度划分数据。Hive是针对某一列进行分桶。Hive采用对列值哈希，然后除以桶的个数求余的方式决定该条记录存放在哪个桶中。分桶的好处是可以获得更高的查询处理效率。使取样更高效。

​	当从桶表中进行查询时，hive会根据分桶的字段进行计算分析出数据存放的桶中，然后直接到对应的桶中去取数据，这样做就很好的提高了效率。

### HBase

#### HBase的特点	

​	HBase是一个构建在HDFS上的分布式列存储系统；

​	面向列：面向列（族）的存储和权限控制，列（族）独立检索；

​	稀疏：空（null）列并不占用存储空间，表可以设计的非常稀疏； 数据多版本：每个单元中的数据可以有多个版本，默认情况下版本号自动分配，是单元格插入时的时间戳； 数据类型单一：Hbase中的数据都是字符串，没有类型。

#### HBase的架构图

![HBase架构图](/Users/youyujie/Documents/Java知识点复习图片/HBase架构图.png)

HBase的各个基本组件作用：

**Master：**

​	1.为Region Server分配region

​	2.负责Region Server的负载均衡

​	3.发现失效的Region Server并重新分配其上的region

​	4.管理用户对table的增删改查操作

**Region Server：**

​	1.维护region，负责对region的IO请求

​	2.负责切分在运行过程中变大的region

**Zookeeper：**

​	1.通过选举，保证任何时候，集群中只有一个master，Master与RegionServers 启动时会向ZooKeeper注册

​	2.存贮所有Region的寻址入口

​	3.实时监控Region server的上线和下线信息。并实时通知给Master

​	4.存储HBase的schema和table元数据

​	5.默认情况下，HBase 管理ZooKeeper 实例，比如， 启动或者停止ZooKeeper

​	6.Zookeeper的引入使得Master不再是单点故障

**Write-Ahead-Log（WAL）：**

![HBase的WAL](/Users/youyujie/Documents/Java知识点复习图片/HBase的WAL.png)

​	每个HRegionServer中都有一个HLog对象，HLog是一个实现Write Ahead Log的类，在每次用户操作写入MemStore的同时，也会写一份数据到HLog文件中（HLog文件格式见后续），HLog文件定期会滚动出新的，并删除旧的文件（已持久化到StoreFile中的数据）。当HRegionServer意外终止后，HMaster会通过Zookeeper感知到，HMaster首先会处理遗留的 HLog文件，将其中不同Region的Log数据进行拆分，分别放到相应region的目录下，然后再将失效的region重新分配，领取 到这些region的HRegionServer在Load Region的过程中，会发现有历史HLog需要处理，因此会Replay HLog中的数据到MemStore中，然后flush到StoreFiles，完成数据恢复。

#### Region定位的流程

![Region 定位流程](/Users/youyujie/Documents/Java知识点复习图片/Region 定位流程.png)

**寻找RegionServer**

​	ZooKeeper--> -ROOT-(单Region)--> .META.--> 用户表

**-ROOT-**

 	表包含.META.表所在的region列表，该表只会有一个Region；

 	Zookeeper中记录了-ROOT-表的location。

**.META.**

​	表包含所有的用户空间region列表，以及RegionServer的服务器地址。

#### HBase写流程

​	1、   client向hregionserver发送写请求。

​	2、   hregionserver将数据写到hlog（write ahead log）。为了数据的持久化和恢复。

​	3、   hregionserver将数据写到内存（memstore）

​	4、   反馈client写成功。

#### 数据Flush过程

​	1、 当memstore数据达到阈值（默认是64M），将数据刷到硬盘，将内存中的数据删除，同时删除Hlog中的历史数据。

​	2、并将数据存储到hdfs中。

​	3、在hlog中做标记点。

#### 数据合并过程

​	1、   通过zookeeper和-ROOT- .META.表定位hregionserver。

​	2、   数据从内存和硬盘合并后返回给client

​	3、   数据块会缓存

## 实时计算篇

### Storm

#### Storm的特点

​	数据实时产生、数据实时传输、数据实时计算、实时展示 。

#### Storm核心

![Storm核心组件](/Users/youyujie/Documents/Java知识点复习图片/Storm核心组件.png) 

Nimbus：负责资源分配和任务调度。 

Supervisor：负责接受nimbus分配的任务，启动和停止属于自己管理的worker进程。**---通过配置文件设置当前supervisor上启动多少个worker。** 

Worker：运行具体处理组件逻辑的进程。Worker运行的任务类型只有两种，一种是Spout任务，一种是Bolt任务 。

Task：worker中每一个spout/bolt的线程称为一个task. 在storm0.8之后，task不再与物理线程对应，不同spout/bolt的task可能会共享一个物理线程，该线程称为executor。 

#### Storm编程模型

![image-20180530154018980](/Users/youyujie/Documents/Java知识点复习图片/Storm编程模型.png)

DataSource：外部数据源
	Spout：接受外部数据源的组件，将外部数据源转化成Storm内部的数据，以Tuple为基本的传输单元下发给Bolt
	Bolt:接受Spout发送的数据，或上游的bolt的发送的数据。根据业务逻辑进行处理。发送给下一个Bolt或者是存储到某种介质上。介质可以是Redis可以是mysql，或者其他。
	Tuple：Storm内部中数据传输的基本单元，里面封装了一个List对象，用来保存数据。
	StreamGrouping:数据分组策略
	7种：shuffleGrouping(Random函数),Non Grouping(Random函数),FieldGrouping(Hash取模)、Local or ShuffleGrouping 本地或随机，优先本地。	

#### Storm并发

​	在Storm集群上运行的拓扑主要包含以下的三个实体：1、Worker进程 2、Executors 3、Tasks(任务)

![image-20180530161818401](/Users/youyujie/Documents/Java知识点复习图片/Storm并发.png)

​	一个正在运行的拓扑由很多worker进程组成，这些worker进程在Storm集群的多台机器上运行。一个worker进程属于一个特定的拓扑并且执行这个拓扑的一个或多个component（spout或者bolt)的一个或多个executor。一个worker进程就是一个Java虚拟机(JVM)，它执行一个拓扑的一个子集。

​	一个executor是由一个worker进程产生的一个线程，它运行在worker的Java虚拟机里。一个executor为同一个component(spout或bolt)运行一个或多个任务。一个executor总会有一个线程来运行executor所有的task,这说明task在executor内部是串行执行的。

​	真正的数据处理逻辑是在task里执行的，在父executor线程执行过程中会运行task。在代码中实现的每个spout或bolt是在全集群中以很多task的形式运行的。一个component的task数量在这个拓扑的生命周期中是固定不变的，但是一个component的executor(线程)数量会随着时间推移发生变化。这说明以下条件一直成立：`threads数量 <= task数量`。默认情况下task数量被设置成跟executor的数量是一样的，即Storm会在每个线程上执行一个任务(这通常是你想要的)。

#### Storm架构

架构
	Nimbus：任务分配
	Supervisor：接受任务，并启动worker。worker的数量根据端口号来的。
	Worker:执行任务的具体组件（其实就是一个JVM）,可以执行两种类型的任务，Spout任务或者bolt任务。
	Task：Task=线程=executor。 一个Task属于一个Spout或者Bolt并发任务。
	Zookeeper：保存任务分配的信息、心跳信息、元数据信息。

![image-20180606154307049](/Users/youyujie/Documents/Java知识点复习图片/Storm架构.png)

#### Worker与topology

​	一个worker只属于一个topology,每个worker中运行的task只能属于这个topology。    反之，一个topology包含多个worker，其实就是这个topology运行在多个worker上。
	一个topology要求的worker数量如果不被满足，集群在任务分配时，根据现有的worker先运行topology。如果当前集群中worker数量为0，那么最新提交的topology将只会被标识active，不会运行，只有当集群有了空闲资源之后，才会被运行。

#### Storm的ack-fail机制

​	Storm中有个特殊的task名叫acker，他们负责跟踪spout发出的每一个Tuple的Tuple树（因为一个tuple通过spout发出了，经过每一个bolt处理后，会生成一个新的tuple发送出去）。当acker（框架自启动的task）发现一个Tuple树已经处理完成了，它会发送一个消息给产生这个Tuple的那个task。
Acker的跟踪算法是Storm的主要突破之一，对任意大的一个Tuple树，它只需要恒定的20字节就可以进行跟踪。
Acker跟踪算法的原理：acker对于每个spout-tuple保存一个ack-val的校验值，它的初始值是0，然后每发射一个Tuple或Ack一个Tuple时，这个Tuple的id就要跟这个校验值异或一下，并且把得到的值更新为ack-val的新值。那么假设每个发射出去的Tuple都被ack了，那么最后ack-val的值就一定是0。Acker就根据ack-val是否为0来判断是否完全处理，如果为0则认为已完全处理。

要实现ack机制：
1，spout发射tuple的时候指定messageId
2，spout要重写BaseRichSpout的fail和ack方法
3，spout对发射的tuple进行缓存(否则spout的fail方法收到acker发来的messsageId，spout也无法获取到发送失败的数据进行重发)，看看系统提供的接口，只有msgId这个参数，这里的设计不合理，其实在系统里是有cache整个msg的，只给用户一个messageid，用户如何取得原来的msg貌似需要自己cache，然后用这个msgId去查询，
3，spout根据messageId对于ack的tuple则从缓存队列中删除，对于fail的tuple可以选择重发。
4,设置acker数至少大于0；Config.setNumAckers(conf, ackerParal);

#### 如何开启Storm ack机制

- spout端发送数据的时候，要加上messageid.
- spout要重写ack和fail方法。
- 在topologybuiler的config文件中设置setNumAckers的数量大于1.默认是1
- 在下游个每个层级bolt上，需要增加锚点。

#### Storm WordCount程序代码

```java
package cn.edu.hust.storm;

import org.apache.storm.Config;
import org.apache.storm.LocalCluster;
import org.apache.storm.StormSubmitter;
import org.apache.storm.generated.AlreadyAliveException;
import org.apache.storm.generated.AuthorizationException;
import org.apache.storm.generated.InvalidTopologyException;
import org.apache.storm.generated.LocalAssignment;
import org.apache.storm.topology.TopologyBuilder;
import org.apache.storm.tuple.Fields;

public class WordCount {
    public static void main(String[] args) throws InvalidTopologyException, AuthorizationException, AlreadyAliveException {
        TopologyBuilder builder =new TopologyBuilder();
        //设置spout
        builder.setSpout("MySpout",new MySpout(),3);
        //设置bolt，并且设置并行度，设置上一个spout的分区策略
        builder.setBolt("SplitBolt",new SplitBolt(),10).shuffleGrouping("MySpout");
        //设置bolt，并且设置并行度，设置上一个bolt的分区策略
        builder.setBolt("CountBolt",new CountBolt(),5).fieldsGrouping("SplitBolt",new Fields("word"));

        Config conf=new Config();
        conf.setNumWorkers(6);

        //使用集群模式提交任务
        StormSubmitter.submitTopology(args[0],conf,builder.createTopology());
        //使用本地模式提交任务
       /* LocalCluster localCluster=new LocalCluster();
        localCluster.submitTopology("wordcount",conf,builder.createTopology());*/
    }
}


package cn.edu.hust.storm;

import org.apache.storm.spout.SpoutOutputCollector;
import org.apache.storm.task.TopologyContext;
import org.apache.storm.topology.IRichSpout;
import org.apache.storm.topology.OutputFieldsDeclarer;
import org.apache.storm.topology.base.BaseRichSpout;
import org.apache.storm.tuple.Fields;
import org.apache.storm.tuple.Values;

import java.util.Map;

public class MySpout extends BaseRichSpout {
    private SpoutOutputCollector spoutOutputCollector;
    public void open(Map map, TopologyContext topologyContext, SpoutOutputCollector spoutOutputCollector) {
        this.spoutOutputCollector=spoutOutputCollector;
    }

    public void nextTuple() {
        spoutOutputCollector.emit(new Values("hello this is my first storm progress"));
    }

    public void declareOutputFields(OutputFieldsDeclarer outputFieldsDeclarer) {
        outputFieldsDeclarer.declare(new Fields("sentence"));
    }
}


package cn.edu.hust.storm;

import org.apache.storm.spout.SpoutOutputCollector;
import org.apache.storm.task.OutputCollector;
import org.apache.storm.task.TopologyContext;
import org.apache.storm.topology.IBasicBolt;
import org.apache.storm.topology.IRichBolt;
import org.apache.storm.topology.IWindowedBolt;
import org.apache.storm.topology.OutputFieldsDeclarer;
import org.apache.storm.topology.base.BaseRichBolt;
import org.apache.storm.topology.base.BaseRichSpout;
import org.apache.storm.tuple.Fields;
import org.apache.storm.tuple.Tuple;
import org.apache.storm.tuple.Values;

import java.util.Map;

public class SplitBolt extends BaseRichBolt{
    private OutputCollector outputCollector;
    public void prepare(Map map, TopologyContext topologyContext, OutputCollector outputCollector) {
        this.outputCollector=outputCollector;
    }


    public void execute(Tuple tuple) {
        //通过下标去数据
        String line=tuple.getString(0);
        String[] splits=line.split(" ");
        for (String word:
             splits) {
            outputCollector.emit(new Values(word,1));
        }
    }

    public void declareOutputFields(OutputFieldsDeclarer outputFieldsDeclarer) {

        outputFieldsDeclarer.declare(new Fields("word","num"));
    }



}


package cn.edu.hust.storm;

import org.apache.storm.task.OutputCollector;
import org.apache.storm.task.TopologyContext;
import org.apache.storm.topology.IBasicBolt;
import org.apache.storm.topology.IRichBolt;
import org.apache.storm.topology.IWindowedBolt;
import org.apache.storm.topology.OutputFieldsDeclarer;
import org.apache.storm.topology.base.BaseRichBolt;
import org.apache.storm.tuple.Fields;
import org.apache.storm.tuple.Tuple;

import java.util.HashMap;
import java.util.Map;

public class CountBolt extends BaseRichBolt{
    private OutputCollector outputCollector;
    private Map<String,Integer> words=new HashMap<String, Integer>();
    public void prepare(Map map, TopologyContext topologyContext, OutputCollector outputCollector) {
        this.outputCollector=outputCollector;
    }

    public void execute(Tuple tuple) {
        String word=tuple.getString(tuple.fieldIndex("word"));
        int num=1;
        num=tuple.getInteger(tuple.fieldIndex("num"));
        if (words.containsKey(word))
        {
            words.put(word,words.get(word)+num);
        }
        else
        {
            words.put(word,num);
        }
        System.out.println(words);
    }

    public void declareOutputFields(OutputFieldsDeclarer outputFieldsDeclarer) {
        outputFieldsDeclarer.declare(new Fields("word"));
    }
}
```

#### Storm面试题详解

​	问题：
	1，kafka+storm如何保证消息完整处理。		
		一条消息产生----Kafka--KafkaSpout-Storm--->Redis
		问题1：kafka数据生产消费如何保证消息的完整处理
			Producer-batch(缓存机制queue)--重试机制---->ack(-1,1,0)---Broker(partition leader/slave)------>KafkaConsumer(内存中，new offset)---->zk( old offset)
		Producer发送时缓存数据，需要阈值设定（数量阈值，时间阈值），当数据太多并来不及发送时，会产生老数据是否保留的问题，如何配置文件中配置的是删除掉，数据就丢失了。
		

		KafkaConsumer 消费数据时，由于offset是周期性更新，导致zk上的offset值必然小于Kafkaconsumer内存中的值，当KafkaConsumer挂掉后重启，必然会导致数据重复消费。
		如何解决重复消费：1，技术方法(不可取)，2，业务方法（标识数据：找到消息中的唯一标识 <messageTag,isProcess>）


​			
		问题2：Storm中如何保证消息的完整处理  ack - fail 
		 自己定义 缓存Map<msgid,messageObj>
		 Spout---->nextTuple----Tuple---Bolt(ack(tuple))
										Bolt1-----Tuple1-1
										Bolt1-----Tuple1-2
										Bolt1-----Tuple1-3
										Bolt1-----Tuple1-4
													Bolt2(ack(Tuple1-1))
													Bolt2(ack(Tuple1-2))
													Bolt2(ack(Tuple1-3))
													Bolt2(ack(Tuple1-4))
													
		 如果成功（spout.ack(msg)）
			map.remove(msgid)
		 如果失败（spout.fail(msg)）
			messageObj = map.get(msgid)
			collector.emit(messageObj,msgid)
	
		问题3：如何保证一条消息路过各个组件时保证全局的完整处理
			保存每个环节的数据不丢失，自然就全局不丢失。


​		
		问题4：在storm环节中自定义的map如何存储，在KafkaConsumer如何处理重复消费的问题？
			1，保存在当前Jvm中，既然出异常导致消息不能完全处理，存放在jvm中的标识数据，缓存Map必然会丢失
			2，保存在外部的存储中，redis。  标识数据<Set>  缓存Map<Map> 
				问题：请问存储redis时，由于网络原因或其他异常导致数据不能成功存储 怎么办？
				重试机制，保存3次，打印log日志，redis存储失败。
	
	2，数据量大如何保证到Kakfa中，storm如何消费，解决延迟。
		问题1：请问Kafka如何处理海量数据       -----整体数据：100G
			  数据从何而来：producer 集群----DefaultPartition  ------保存数据平均分配
			  
			  数据保存在哪里： broker 集群 
								topic ---partition  10   -------------->每个分片保存10G 
			  数据保存如何快：pageCache 600M/S的磁盘写入速度  sendfile技术
	
		问题2: 请问Storm如何处理海量数据，尽可能快
			  数据输入： KafkaSpout(consumerGroup,10) 读取外部数据源
			  数据计算： 数据计算是根据对数据处理的业务复杂度来的，越复杂并发度越大。
			             如果bolt的并发读要设置成1万个，才能提高处理速度，很显然是不行的。需要将bolt中的代码逻辑分解出来，形成多个bolt组合。
						 bolt1--->bolt2--->bolt3......			 
						 
	3，Flume监控文件，异常之后重新启动，如何避免重复读取。
	   异常的范围是 flumeNg的异常
	   log文件，读取文件，保存记录的行号
	   问题1：如何保存行号
			command.type = exec shell<tail -F  xxx.log>  自定义shell脚本，保存消费的行号到工作目录的某个文件里。
			当下次启动时，从行号记录文件中拿取上次读到多少行。		 


​			 

### Kafka

#### Kafka基础知识

![image-20180606160911303](/Users/youyujie/Documents/Java知识点复习图片/Kafka基础节点.png)

#### Kafka知识总结

1、kafka是什么
	类JMS消息队列，结合JMS中的两种模式，可以有多个消费者主动拉取数据，在JMS中只有点对点模式才有消费者主动拉取数据。
	kafka是一个生产-消费模型。
	Producer：生产者，只负责数据生产，生产者的代码可以集成到任务系统中。 
			  数据的分发策略由producer决定，默认是defaultPartition  Utils.abs(key.hashCode) % numPartitions
	Broker：当前服务器上的Kafka进程,俗称拉皮条。只管数据存储，不管是谁生产，不管是谁消费。
			在集群中每个broker都有一个唯一brokerid，不得重复。
	Topic:目标发送的目的地，这是一个逻辑上的概念，落到磁盘上是一个partition的目录。partition的目录中有多个segment组合(index,log)
	一个Topic对应多个partition[0,1,2,3]，一个partition对应多个segment组合。一个segment有默认的大小是1G。
	每个partition可以设置多个副本(replication-factor 1),会从所有的副本中选取一个leader出来。所有读写操作都是通过leader来进行的。
	特别强调，和mysql中主从有区别，mysql做主从是为了读写分离，在kafka中读写操作都是leader。
	ConsumerGroup：数据消费者组，ConsumerGroup可以有多个，每个ConsumerGroup消费的数据都是一样的。
	 可以把多个consumer线程划分为一个组，组里面所有成员共同消费一个topic的数据，组员之间不能重复消费。
				   
2、kafka生产数据时的分组策略
	默认是defaultPartition  Utils.abs(key.hashCode) % numPartitions
	上文中的key是producer在发送数据时传入的，produer.send(KeyedMessage(topic,myPartitionKey,messageContent))

3、kafka如何保证数据的完全生产
	ack机制：broker表示发来的数据已确认接收无误，表示数据已经保存到磁盘。
	0：不等待broker返回确认消息
	1：等待topic中某个partition leader保存成功的状态反馈
	-1：等待topic中某个partition 所有副本都保存成功的状态反馈
	
4、broker如何保存数据
	在理论环境下，broker按照顺序读写的机制，可以每秒保存600M的数据。主要通过pagecache机制，尽可能的利用当前物理机器上的空闲内存来做缓存。
	当前topic所属的broker，必定有一个该topic的partition，partition是一个磁盘目录。partition的目录中有多个segment组合(index,log)

5、partition如何分布在不同的broker上
	int i = 0
	list{kafka01,kafka02,kafka03}
	
	for(int i=0;i<5;i++){
		brIndex = i%broker;
		hostName = list.get(brIndex)
	}

​	

6、consumerGroup的组员和partition之间如何做负载均衡
	最好是一一对应，一个partition对应一个consumer。
	如果consumer的数量过多，必然有空闲的consumer。

	算法：
		假如topic1,具有如下partitions: P0,P1,P2,P3
		加入group中,有如下consumer: C1,C2
		首先根据partition索引号对partitions排序: P0,P1,P2,P3
		根据consumer.id排序: C0,C1
		计算倍数: M = [P0,P1,P2,P3].size / [C0,C1].size,本例值M=2(向上取整)
		然后依次分配partitions: C0 = [P0,P1],C1=[P2,P3],即Ci = [P(i * M),P((i + 1) * M -1)]

7、如何保证kafka消费者消费数据是全局有序的
	伪命题
	如果要全局有序的，必须保证生产有序，存储有序，消费有序。
	由于生产可以做集群，存储可以分片，消费可以设置为一个consumerGroup，要保证全局有序，就需要保证每个环节都有序。
	只有一个可能，就是一个生产者，一个partition，一个消费者。这种场景和大数据应用场景相悖。

8.如何实现消息的均衡分发?

​	 消息由producer直接通过socket发送到broker，中间不会经过任何"路由层"，事实上，消息被路由到哪个partition上由producer客户端决定；

​	比如可以采用"random""key-hash""轮询"等,**如果一个topic****中有多个partitions,那么在producer端实现消息均衡分发是必要的。

​	在producer端的配置文件中,开发者可以指定partition路由的方式。 

#### Kafka结构	

- **Broker**
  　　Kafka集群包含一个或多个服务器，这种服务器被称为broker
- **Topic**
  　　每条发布到Kafka集群的消息都有一个类别，这个类别被称为Topic。（物理上不同Topic的消息分开存储，逻辑上一个Topic的消息虽然保存于一个或多个broker上但用户只需指定消息的Topic即可生产或消费数据而不必关心数据存于何处）
- **Partition**
  　　Parition是物理上的概念，每个Topic包含一个或多个Partition.
- **Producer**
  　　负责发布消息到Kafka broker
- **Consumer**
  　　消息消费者，向Kafka broker读取消息的客户端。
- **Consumer Group**
  　　每个Consumer属于一个特定的Consumer Group（可为每个Consumer指定group name，若不指定group name则属于默认的group）。

####   Kafka Partition消息读写

​	因为每条消息都被append到该Partition中，属于顺序写磁盘，因此效率非常高（经验证，顺序写磁盘效率比随机写内存还要高，这是Kafka高吞吐率的一个很重要的保证）。

![Kafka消息读写](/Users/youyujie/Documents/Java知识点复习图片/Kafka消息读写.png)

​	　对于传统的message queue而言，一般会删除已经被消费的消息，而Kafka集群会保留所有的消息，无论其被消费与否。当然，因为磁盘限制，不可能永久保留所有数据（实际上也没必要），因此Kafka提供两种策略删除旧数据。一是基于时间，二是基于Partition文件大小。

​	  这里要注意，因为Kafka读取特定消息的时间复杂度为O(1)，即与文件大小无关，所以这里删除过期文件与提高Kafka性能无关。选择怎样的删除策略只与磁盘以及具体的需求有关。另外，Kafka会为每一个Consumer Group保留一些metadata信息——当前消费的消息的position，也即offset。这个offset由Consumer控制。正常情况下Consumer会在消费完一条消息后递增该offset。当然，Consumer也可将offset设成一个较小的值，重新消费一些消息。因为offset由Consumer控制，所以Kafka broker是无状态的，它不需要标记哪些消息被哪些消费过，也不需要通过broker去保证同一个Consumer Group只有一个Consumer能消费某一条消息，因此也就不需要锁机制，这也为Kafka的高吞吐率提供了有力保障。 　

#### Kafka文件存储基本机构

​	在Kafka文件存储中，同一个topic下有多个不同partition，每个partition为一个目录，partiton命名规则为topic名称+有序序号，第一个partiton序号从0开始，序号最大值为partitions数量减1。 

​	 每个partion(目录)相当于一个巨型文件被平均分配到多个大小相等segment(段)数据文件中。**但每个段****segment file消息数量不一定相等**，这种特性方便old segment file快速被删除。默认保留7天的数据。

![Kafka文件存储基本结构](/Users/youyujie/Documents/Java知识点复习图片/Kafka文件存储基本结构.png)

​	每个partiton只需要支持顺序读写就行了，segment文件生命周期由服务端配置参数决定。（什么时候创建，什么时候删除） 

### Spark

#### Spark的特点

​	1.快：Spark基于内存，比MapReduce快100倍以上

​	2.易用：支持多语言API，可以支持不同的用户。

​	3.通用：提供统一解决方案，用于提供批处理、交互式查询、机器需学习和图计算。

​	4.兼容性好：非常方便与其他开源产品进行融合。

#### Spark的WorkCount程序

```scala
val conf=new SparkConf().setAppName("WC")
val sc=new SparkContext(conf)
sc.textFile("").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).saveAsTextFile("")
```

#### Spark计算模型

​	RDD:分布式数据集，是Spark代表一个不可辨、可分区、里面的元素可并行计算的集合。

RDD的属性：

1.一组分片，即数据集的基本组成单位。对于RDD来说，每个分片都会被一个计算任务处理，并决定并行计算的力度。用户可以在创建RDD时指定RDD的分片个数，如果没有指定，那么将会采用默认值，默认值时程序分配的CPU Core的数量。

2.一个计算每个分区的函数。Spark中RDD的计算是以分片为单位的，每个RDD都会实现compute函数以达到这个目的。compute函数会对迭代器进行复合，不需要保存每次计算的结果。

3.RDD之间的依赖关系。RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系1）重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。 

4.一个Partitioner，即RDD的分片函数。当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。只有对于于key-value的RDD，才会有Partitioner，非key-value的RDD的Parititioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。 

5.一个列表，存储存取每个Partition的优先位置（preferred location）。对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置。 

​	RDD可分为两类，一类是Transformation，一类是Action，

Transformtion：

​	RDD中的所有转换都是延迟加载的，也就是说，它们并不会直接计算结果。相反的，它们只是记住这些应用到基础数据集（例如一个文件）上的转换动作。只有当发生一个要求返回结果给Driver的动作时，这些转换才会真正运行。这种设计让Spark更加有效率地运行。 

Action：

​	会计算出结果。

#### RDD的依赖关系

​	RDD和它依赖的父RDD（s）的关系有两种不同的类型，即窄依赖（narrow dependency）和宽依赖（wide dependency）。 

![RDD依赖关系](/Users/youyujie/Documents/Java知识点复习图片/RDD依赖关系.png)

窄依赖：

​	窄依赖指的是每一个父RDD的Partition最多被子RDD的一个Partition使用

总结：窄依赖我们形象的比喻为**独生子女**

宽依赖：

​	宽依赖指的是多个子RDD的Partition会依赖同一个父RDD的Partition

总结：窄依赖我们形象的比喻为**超生**

#### RDD的缓存

​	Spark速度非常快的原因之一，就是在不同操作中可以在内存中持久化或缓存个数据集。当持久化某个RDD后，每一个节点都将把计算的分片结果保存在内存中，并在对此RDD或衍生出的RDD进行的其他动作中重用。这使得后续的动作变得更加迅速。RDD相关的持久化和缓存，是Spark最重要的特征之一。可以说，缓存是Spark构建迭代式算法和快速交互式查询的关键。 

RDD缓存的方法有：

persist方法和cache方法，只用触发后面的action时，才会缓存。

#### Lineage

​	RDD只支持粗粒度转换，即在大量记录上执行的单个操作。将创建RDD的一系列Lineage（即血统）记录下来，以便恢复丢失的分区。RDD的Lineage会记录RDD的元数据信息和转换行为，当该RDD的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区。 

#### DAG的生成

DAG(Directed Acyclic Graph)叫做有向无环图，原始的RDD通过一系列的转换就就形成了DAG，根据RDD之间的依赖关系的不同将DAG划分成不同的Stage，对于窄依赖，partition的转换处理在Stage中完成计算。对于宽依赖，由于有Shuffle的存在，只能在parent RDD处理完成后，才能开始接下来的计算，因此宽依赖是划分Stage的依据。 

![DAG的生成](/Users/youyujie/Documents/Java知识点复习图片/DAG的生成.png)

#### Spark任务执行流程

![image-20180605171549135](/Users/youyujie/Documents/Java知识点复习图片/Spark任务执行流程.png)

Master：负责资源分配，监控Wroker

Worker：向Master报活，创建Executor执行任务

Executor:执行Driver划分的Task任务

Driver：提交任务的客户端叫做Driver，负责Stage的划分，主要的依据是宽依赖。

#### Spark SQL

​	Spark SQL是Spark用来处理结构化数据的一个模块，它提供了一个编程抽象叫做DataFrame并且作为分布式SQL不会查询引擎的作用。

Spark SQL的特点：易整合、统一的数据访问方式、兼容Hive、标准的数据连接。

##### DataFrames

​	与RDD类似，DataFrame也是一个分布式数据容器。然而DataFrame更像传统数据库的二维表格，除了数据以外，还记录数据的结构信息，即schema。同时，与Hive类似，DataFrame也支持嵌套数据类型（struct、array和map）。从API易用性的角度上 看，DataFrame API提供的是一套高层的关系操作，比函数式的RDD API要更加友好，门槛更低。由于与R和Pandas的DataFrame类似，Spark DataFrame很好地继承了传统单机数据分析的开发体验。 

![DataFrame](/Users/youyujie/Documents/Java知识点复习图片/DataFrame.png)

​	在Spark SQL中SQLContext是床架 DataFrames和执行SQL的入口，在Spark-1.5.2已经内置了SqlContext。

DataFrame常用操作

1.DSL风格语法

2.SQL风格语法 ：需要将DataFrame注册成表

##### Spark SQL编程

实现代码如下：

```scala
package cn.itcast.spark.sql

import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.sql.SQLContext

object InferringSchema {
  def main(args: Array[String]) {

    //创建SparkConf()并设置App名称
    val conf = new SparkConf().setAppName("SQL-1")
    //SQLContext要依赖SparkContext
    val sc = new SparkContext(conf)
    //创建SQLContext
    val sqlContext = new SQLContext(sc)

    //从指定的地址创建RDD
    val lineRDD = sc.textFile(args(0)).map(_.split(" "))

    //创建case class
    //将RDD和case class关联
    val personRDD = lineRDD.map(x => Person(x(0).toInt, x(1), x(2).toInt))
    //导入隐式转换，如果不到人无法将RDD转换成DataFrame
    //将RDD转换成DataFrame
    import sqlContext.implicits._
    val personDF = personRDD.toDF
    //注册表
    personDF.registerTempTable("t_person")
    //传入SQL
    val df = sqlContext.sql("select * from t_person order by age desc limit 2")
    //将结果以JSON的方式存储到指定位置
    df.write.json(args(1))
    //停止Spark Context
    sc.stop()
  }
}
//case class一定要放到外面
case class Person(id: Int, name: String, age: Int)


```

#### Spark Streaming

​	Spark Streaming类似于Apache Storm，用于流式数据的处理。根据其官方文档介绍，Spark Streaming有高吞吐量和容错能力强等特点。Spark Streaming支持的数据输入源很多，例如：Kafka、Flume、Twitter、ZeroMQ和简单的TCP套接字等等。数据输入后可以用Spark的高度抽象原语如：map、reduce、join、window等进行运算。而结果也能保存在很多地方，如HDFS，数据库等。另外Spark Streaming也能和MLlib（机器学习）以及Graphx完美融合。 

![Spark Streaming](/Users/youyujie/Documents/Java知识点复习图片/Spark Streaming.png)

Spark Streaming  的特点

1.易用 2.容错 3.易整合到Spark体系
